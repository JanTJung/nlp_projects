{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanTJung/nlp_projects/blob/main/Intent_slot_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adp5BudsY7Zo"
      },
      "source": [
        "# Implementation of NLU component of a chatbot\n",
        "\n",
        "In this assignment I built an application that performs __intent classification__ and __slot filling__, which are the core tasks that take care the **NLU module** in any chatbot. \n",
        "\n",
        "*   __intent classification__ consist in mapping user utterance (typically a \n",
        "customer) into a predefined set of _intents_ (goals of the user) so the computer can know what she is asking.\n",
        "\n",
        "* __slot filling__ consist in extracting filler values from the uterance of the user. \n",
        "\n",
        "\n",
        "We will use the [ATIS (Airline Travel Information Systems) dataset](https://www.aclweb.org/anthology/H90-1021.pdf), which is  a dataset consisting of audio recordings and corresponding manual transcripts about humans asking for flight information on automated airline travel inquiry systems. The data consists of 17 unique intent categories, and 81 slots in total. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXI8V9eqEdo2",
        "outputId": "4e9fb358-bd9e-4c64-e5cc-f519198be746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import spacy\n",
        "import collections\n",
        "\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "from keras.models import model_from_json\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "BHPmrpHQ3Lxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LiuzPmPZnBC"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "The data is distributed in three files for train, development and testing. Each have a sentence, which represents the user input (the question), following the label for each word in the sentence and the intent, which represents the intention of the question.\n",
        "\n",
        "The labels for the words are about important information, like the airport name, the flight time etc. Words without a influence are labeled with a 0.\n",
        "\n",
        "The distribution of training, development and test sentences is as follows:\n",
        "\n",
        "- Training set: 4978\n",
        "- Development set: 500\n",
        "- Test set: 893\n",
        "\n",
        "In the further code some example sentences, related labels and intents are printed out. Furthermore the distribution over all intents is printed. With that you can see that there most of the intents are \"atis_flight\" with over 75% of the intents.\n",
        "\n",
        "To see the distribution better, the train intents are plotted in their distribution. (Just the train data, as the other ones have similar distribution)\n",
        "\n",
        "Note: I didn't know what you mean by class distribution. I hope the analysis of the data I provided is sufficient.\n",
        "- Class distribution for intent classification\n",
        "- Class distribution for slot filling task. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lulq5OztYDHA"
      },
      "outputs": [],
      "source": [
        "# helper function to read dev and test data format (use get_data2 for training)\n",
        "def get_data(filename):\n",
        "    df = pd.read_csv(filename, delim_whitespace=True, names=['word', 'label'])\n",
        "    beg_indices = list(df[df['word'] == 'BOS'].index)+[df.shape[0]]\n",
        "    sents, labels, intents = [], [], []\n",
        "    for i in range(len(beg_indices[:-1])):\n",
        "        sents.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['word'].values)\n",
        "        labels.append(df[beg_indices[i]+1:beg_indices[i+1]-1]['label'].values)\n",
        "        intents.append(df.loc[beg_indices[i+1]-1]['label'])    \n",
        "    return sents, labels, intents\n",
        "\n",
        "# helper function to read training data format\n",
        "def get_data2(filename):\n",
        "    with open(filename) as f:\n",
        "        contents = f.read()\n",
        "    sents, labels, intents = [],[],[]\n",
        "    for line in contents.strip().split('\\n'):\n",
        "        words, labs = [i.split(' ') for i in line.split('\\t')]\n",
        "        sents.append(words[1:-1])\n",
        "        labels.append(labs[1:-1])\n",
        "        intents.append(labs[-1])\n",
        "    return sents, labels, intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwuqHDG_d8Vq"
      },
      "outputs": [],
      "source": [
        "data_dir=\"drive/My Drive/Colab Notebooks/nlp-app-II/data/atis\" \n",
        "train_file= data_dir + '/atis.train.w-intent.iob'\n",
        "dev_file= data_dir + '/atis-2.dev.w-intent.iob'\n",
        "test_file= data_dir + '/atis.test.w-intent.iob'\n",
        "\n",
        "train_texts, train_slots, train_intents = get_data2(train_file)\n",
        "dev_texts, dev_slots, dev_intents = get_data(dev_file)\n",
        "test_texts, test_slots, test_intents = get_data(test_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCTc0U3bhVfK",
        "outputId": "e6ec8cbd-715a-4efa-da20-d4d9a0bd2ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train text: ['what', 'flights', 'are', 'available', 'from', 'pittsburgh', 'to', 'baltimore', 'on', 'thursday', 'morning']\n",
            "Train labels: ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'B-depart_date.day_name', 'B-depart_time.period_of_day']\n",
            "Train intents: atis_flight\n",
            "\n",
            "Development text: ['show' 'me' 'all' 'round' 'trip' 'flights' 'between' 'houston' 'and'\n",
            " 'las' 'vegas']\n",
            "Development labels: ['O' 'O' 'O' 'B-round_trip' 'I-round_trip' 'O' 'O' 'B-fromloc.city_name'\n",
            " 'O' 'B-toloc.city_name' 'I-toloc.city_name']\n",
            "Development intents: atis_flight\n",
            "\n",
            "Test text: ['i' 'would' 'like' 'to' 'find' 'a' 'flight' 'from' 'charlotte' 'to' 'las'\n",
            " 'vegas' 'that' 'makes' 'a' 'stop' 'in' 'st.' 'louis']\n",
            "Test labels: ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B-fromloc.city_name' 'O'\n",
            " 'B-toloc.city_name' 'I-toloc.city_name' 'O' 'O' 'O' 'O' 'O'\n",
            " 'B-stoploc.city_name' 'I-stoploc.city_name']\n",
            "Test intents: atis_flight\n",
            "\n",
            "Total number of:\n",
            "Training set: 4978\n",
            "Dev set: 500\n",
            "Test set: 893\n",
            "\n",
            "Distribution of intents in train set: [('atis_flight', 3666), ('atis_airfare', 423), ('atis_ground_service', 255), ('atis_airline', 157), ('atis_abbreviation', 147)]\n",
            "Distribution of intents in development set: [('atis_flight', 357), ('atis_airfare', 38), ('atis_ground_service', 25), ('atis_airline', 18), ('atis_abbreviation', 17)]\n",
            "Distribution of intents in test set: [('atis_flight', 632), ('atis_airfare', 48), ('atis_airline', 38), ('atis_ground_service', 36), ('atis_abbreviation', 33)]\n"
          ]
        }
      ],
      "source": [
        "# Analysis of train\n",
        "print(\"Train text: \" + str(train_texts[1]))\n",
        "print(\"Train labels: \" + str(train_slots[1]))\n",
        "print(\"Train intents: \" + str(train_intents[1]))\n",
        "print()\n",
        "\n",
        "#Analysis of dev\n",
        "print(\"Development text: \" + str(dev_texts[1]))\n",
        "print(\"Development labels: \" + str(dev_slots[1]))\n",
        "print(\"Development intents: \" + str(dev_intents[1]))\n",
        "print()\n",
        "\n",
        "#Analysis of test\n",
        "print(\"Test text: \" + str(test_texts[0]))\n",
        "print(\"Test labels: \" + str(test_slots[0]))\n",
        "print(\"Test intents: \" + str(test_intents[0]))\n",
        "print()\n",
        "\n",
        "print(\"Total number of:\")\n",
        "print(\"Training set: \" + str(len(train_texts)))\n",
        "print(\"Dev set: \" + str(len(dev_texts)))\n",
        "print(\"Test set: \" + str(len(test_texts)))\n",
        "print()\n",
        "\n",
        "a_counter_train = collections.Counter(train_intents)\n",
        "most_common_train = a_counter_train.most_common(5)\n",
        "print(\"Distribution of intents in train set: \"+ str(most_common_train))\n",
        "\n",
        "a_counter_dev = collections.Counter(dev_intents)\n",
        "most_common_dev = a_counter_dev.most_common(5)\n",
        "print(\"Distribution of intents in development set: \"+ str(most_common_dev))\n",
        "\n",
        "a_counter_test = collections.Counter(test_intents)\n",
        "most_common_test = a_counter_test.most_common(5)\n",
        "print(\"Distribution of intents in test set: \"+ str(most_common_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "intents_counter = a_counter_train.most_common(10)\n",
        "intent_label_train = []\n",
        "intent_number_train = []\n",
        "for x in intents_counter:\n",
        "  intent_label_train.append(x[0])\n",
        "  intent_number_train.append(x[1])\n",
        "\n",
        "print(intent_label_train)\n",
        "print(intent_number_train)\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Distribution of train set intents', fontsize=16)\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(intent_label_train, intent_number_train)\n",
        "ax.tick_params(axis='x', labelrotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "Za6g89IfvXwe",
        "outputId": "4586d5dd-8013-417e-c84a-3586649f97e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['atis_flight', 'atis_airfare', 'atis_ground_service', 'atis_airline', 'atis_abbreviation', 'atis_aircraft', 'atis_flight_time', 'atis_quantity', 'atis_flight#atis_airfare', 'atis_airport']\n",
            "[3666, 423, 255, 157, 147, 81, 54, 51, 21, 20]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKIOEMzZZ4EU"
      },
      "source": [
        "## Preprocess\n",
        "Data for NLP has to be preprocessed. However, the dataset which was provided already had most of the preprocessing already done. Tokenization and lowering the words, as well as the removal of stop words has already been done. \n",
        "I tested applying Lemmatization and Stemming, however these methods also cut significant parts of the labeled words (e.g. Las Vegas became La Vega). As unlabeled words had no influence anyways, i decided to not further preprocess the data. In case the results are not sufficient I would have come back to this part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxAFAsoaMHAd",
        "outputId": "129737e7-0ad8-499b-83fd-63731c23eebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized text: ['what', 'city', 'is', 'the', 'airport', 'mco', 'in']\n",
            "Tokens labels: ['O', 'O', 'O', 'O', 'O', 'B-fromloc.airport_code', 'O']\n",
            "Tokens intents: atis_city\n",
            "Network input for label training: [['what', 'city', 'is', 'the', 'airport', 'mco', 'in'], ['O', 'O', 'O', 'O', 'O', 'B-fromloc.airport_code', 'O']]\n"
          ]
        }
      ],
      "source": [
        "train_set = []\n",
        "dev_set = []\n",
        "\n",
        "for x in range(len(train_texts)):\n",
        "    element = [train_texts[x],train_slots[x]]\n",
        "    train_set.append(element)\n",
        "\n",
        "for x in range(len(dev_texts)):\n",
        "    element = [dev_texts[x],dev_slots[x]]\n",
        "    dev_set.append(element)\n",
        "\n",
        "\n",
        "print(\"Tokenized text: {}\".format(train_set[104][0]))\n",
        "print(\"Tokens labels: {}\".format(train_set[104][1]))\n",
        "print(\"Tokens intents: {}\".format(train_intents[104]))\n",
        "\n",
        "print(\"Network input for label training: {}\".format(train_set[104]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4c1ky1JZ-Kw"
      },
      "source": [
        "## Build your models\n",
        "\n",
        "For builiding the models the approach from the labs has been reused. By adapting the data to the LSTM Network no problems with labeling the words occured for the sequence labeling task. \n",
        "\n",
        "Also for the intent classification a LSTM has been used by just adaption the data to the network.\n",
        "\n",
        "Results of the network are analysed further down in the code.\n",
        "\n",
        "Note: Please note that the code might look a bit strange with several functions dividing the data preparation and model training parts. I know that this is not necessary for a notebook, but as I had big issues with networks error in colab, therefore I run the code on my laptop and afterwards pasted it in here. (Still works)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_labels():\n",
        "\n",
        "    train_set = []\n",
        "    dev_set = []\n",
        "\n",
        "    for x in range(len(train_texts)):\n",
        "        element = [train_texts[x],train_slots[x]]\n",
        "        train_set.append(element)\n",
        "\n",
        "    for x in range(len(dev_texts)):\n",
        "        element = [dev_texts[x],dev_slots[x]]\n",
        "        dev_set.append(element)\n",
        "\n",
        "\n",
        "    print(\"Tokenized text: {}\".format(train_set[104][0]))\n",
        "    print(\"Tokens labels: {}\".format(train_set[104][1]))\n",
        "\n",
        "\n",
        "    train_sentences = [\" \".join(sent[0]) for sent in train_set]\n",
        "    dev_sentences = [\" \".join(sent[0]) for sent in dev_set]\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',lower=False, oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "    ## text to word indices\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    dev_sequences = tokenizer.texts_to_sequences(dev_sentences)\n",
        "\n",
        "    ## padding\n",
        "    train_data_labels = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=40)\n",
        "    dev_data_labels = tf.keras.preprocessing.sequence.pad_sequences(dev_sequences, maxlen=40)\n",
        "\n",
        "\n",
        "    # process labels\n",
        "    labels = set([label for sent in train_set for label in sent[1]])\n",
        "    label2index = {name: i for i, name in enumerate(sorted(labels))}\n",
        "    index2label = {label2index[name]:name for name in label2index}\n",
        "    train_labels = []\n",
        "    for sent in train_set:\n",
        "        train_labels.append([label2index[label] for label in sent[1]])\n",
        "    train_labels = tf.keras.preprocessing.sequence.pad_sequences(train_labels, maxlen=40)\n",
        "\n",
        "    dev_labels = []\n",
        "    for sent in dev_set:\n",
        "        dev_labels.append([label2index[label] for label in sent[1]])\n",
        "    dev_labels = tf.keras.preprocessing.sequence.pad_sequences(dev_labels, maxlen=40)\n",
        "\n",
        "    # one-hot encoding\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "    dev_labels = tf.keras.utils.to_categorical(dev_labels)\n",
        "\n",
        "    return train_data_labels, train_labels, dev_data_labels, dev_labels, labels, tokenizer, index2label, label2index\n"
      ],
      "metadata": {
        "id": "mznPSpcX3IWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_intents():\n",
        "    global train_intents\n",
        "    global dev_intents\n",
        "    train_set = []\n",
        "    dev_set = []\n",
        "\n",
        "    for x in range(len(train_texts)):\n",
        "        element = [train_texts[x], train_intents[x]]\n",
        "        train_set.append(element)\n",
        "\n",
        "    for x in range(len(dev_texts)):\n",
        "        element = [dev_texts[x],dev_intents[x]]\n",
        "        dev_set.append(element)\n",
        "\n",
        "\n",
        "    print(\"Tokenized text: {}\".format(train_set[104][0]))\n",
        "    print(\"Tokenized intent: {}\".format(train_set[104][1]))\n",
        "    #print(\"Tokens intents: {}\".format(train_intents[104]))\n",
        "\n",
        "\n",
        "    train_sentences = [\" \".join(sent[0]) for sent in train_set]\n",
        "    dev_sentences = [\" \".join(sent[0]) for sent in dev_set]\n",
        "\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',lower=False, oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "    ## text to word indices\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    dev_sequences = tokenizer.texts_to_sequences(dev_sentences)\n",
        "\n",
        "    ## padding\n",
        "    train_data_intents = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=40)\n",
        "    dev_data_intents = tf.keras.preprocessing.sequence.pad_sequences(dev_sequences, maxlen=40)\n",
        "\n",
        "\n",
        "    # process intents\n",
        "    intents = set()\n",
        "    for sent in train_set:\n",
        "        intents.add(sent[1])\n",
        "    #intents = set([intent for sent in train_set for intent in sent[1]])\n",
        "    intent2index = {name: i for i, name in enumerate(sorted(intents))}\n",
        "    index2intent = {intent2index[name]:name for name in intent2index}\n",
        "    train_intents = []\n",
        "    for sent in train_set:\n",
        "        train_intents.append([intent2index[sent[1]]])\n",
        "\n",
        "    train_intents = tf.keras.preprocessing.sequence.pad_sequences(train_intents, maxlen=40)\n",
        "\n",
        "    dev_intents = []\n",
        "    for sent in dev_set:\n",
        "        dev_intents.append([intent2index[sent[1]]])\n",
        "    dev_intents = tf.keras.preprocessing.sequence.pad_sequences(dev_intents, maxlen=40)\n",
        "\n",
        "    # one-hot encoding\n",
        "    train_intents = tf.keras.utils.to_categorical(train_intents)\n",
        "    dev_intents = tf.keras.utils.to_categorical(dev_intents)\n",
        "\n",
        "    return train_data_intents, train_intents, dev_data_intents, dev_intents, intents, tokenizer, index2intent, intent2index\n"
      ],
      "metadata": {
        "id": "sWDjWuQM3IjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_label_model(train_data_labels, train_labels, dev_data_labels, dev_labels, labels, tokenizer):\n",
        "    \n",
        "    learning_rate = 0.01\n",
        "    epochs = 5\n",
        "    num_classes = len(labels)\n",
        "    batch_size = 128\n",
        "    optimizer = 'adam'\n",
        "    seed = 42\n",
        "    vocab_size = len(tokenizer.index_word)+1\n",
        "    emb_size = 300\n",
        "    lstm_size = 128\n",
        "    dropout = 0.2\n",
        "\n",
        "    # model\n",
        "    model = tf.keras.models.Sequential([\n",
        "                tf.keras.layers.Input(shape=(None,), dtype='int32', name='word_ids'),\n",
        "                tf.keras.layers.Embedding(vocab_size, emb_size,\n",
        "                                          mask_zero=True, trainable=True),\n",
        "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=lstm_size,\n",
        "                                                                   return_sequences=True)),\n",
        "                tf.keras.layers.Dropout(dropout),\n",
        "                tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "               ])\n",
        "\n",
        "    # compile model with ADAM optimizer\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, ),\n",
        "                  loss= \"categorical_crossentropy\",\n",
        "                  metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
        "\n",
        "    model.fit(train_data_labels, train_labels,\n",
        "          batch_size=batch_size, epochs=epochs,\n",
        "          validation_data=(dev_data_labels, dev_labels) , callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "RCpDDHIE3ImU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_intents_model(train_data_intents, train_intents, dev_data_intents, dev_intents, intents, tokenizer):\n",
        "    \n",
        "    learning_rate = 0.01\n",
        "    epochs = 5\n",
        "    num_classes = len(intents)\n",
        "    batch_size = 128\n",
        "    optimizer = 'adam'\n",
        "    seed = 42\n",
        "    vocab_size = len(tokenizer.index_word) + 1\n",
        "    emb_size = 300\n",
        "    lstm_size = 128\n",
        "    dropout = 0.2\n",
        "\n",
        "    # model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Input(shape=(None,), dtype='int32', name='word_ids'),\n",
        "        tf.keras.layers.Embedding(vocab_size, emb_size,\n",
        "                                  mask_zero=True, trainable=True),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=lstm_size,\n",
        "                                                           return_sequences=True)),\n",
        "        tf.keras.layers.Dropout(dropout),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # compile model with ADAM optimizer\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, ),\n",
        "                  loss=\"categorical_crossentropy\",\n",
        "                  metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
        "\n",
        "    model.fit(train_data_intents, train_intents,\n",
        "              batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(dev_data_intents, dev_intents), callbacks=[tf.keras.callbacks.EarlyStopping(patience=2)])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "V1qfay1o3Ip_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(x_test, y_test, model):\n",
        "    preds = model.predict(x_test)\n",
        "    preds = np.argmax(preds, axis=-1)\n",
        "    labs = np.argmax(y_test, axis=-1)\n",
        "    preds = [y[x != 0] for x, y in zip(x_test, preds)]\n",
        "    labs = [y[x != 0] for x, y in zip(x_test, labs)]\n",
        "    return preds, labs"
      ],
      "metadata": {
        "id": "Xo8pvlE53It0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_labels, train_labels, dev_data_labels, dev_labels, labels, tokenizer_label, index2label, label2index = prepare_data_labels()\n",
        "train_data_intents, train_intents, dev_data_intents, dev_intents, intents, tokenizer_intent, index2intent, intent2index = prepare_data_intents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb_G7RKN4-yL",
        "outputId": "3f77529f-0875-4c5d-b8a6-32daf98c0672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized text: ['what', 'city', 'is', 'the', 'airport', 'mco', 'in']\n",
            "Tokens labels: ['O', 'O', 'O', 'O', 'O', 'B-fromloc.airport_code', 'O']\n",
            "Tokenized text: ['what', 'city', 'is', 'the', 'airport', 'mco', 'in']\n",
            "Tokenized intent: atis_city\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the models\n",
        "label_model = train_label_model(train_data_labels, train_labels, dev_data_labels, dev_labels, labels, tokenizer_label)\n",
        "intents_model = train_intents_model(train_data_intents, train_intents, dev_data_intents, dev_intents, intents, tokenizer_intent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sigPaqv76LRh",
        "outputId": "89a7a23f-9b1f-4968-d20c-a1c629441edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "39/39 [==============================] - 35s 649ms/step - loss: 0.3870 - accuracy: 0.7283 - recall: 0.6131 - precision: 0.9046 - val_loss: 0.1442 - val_accuracy: 0.8962 - val_recall: 0.8311 - val_precision: 0.9808\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 23s 592ms/step - loss: 0.0695 - accuracy: 0.9443 - recall: 0.9188 - precision: 0.9789 - val_loss: 0.0399 - val_accuracy: 0.9641 - val_recall: 0.9535 - val_precision: 0.9761\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 21s 547ms/step - loss: 0.0252 - accuracy: 0.9790 - recall: 0.9731 - precision: 0.9850 - val_loss: 0.0195 - val_accuracy: 0.9818 - val_recall: 0.9776 - val_precision: 0.9878\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 21s 548ms/step - loss: 0.0150 - accuracy: 0.9872 - recall: 0.9841 - precision: 0.9905 - val_loss: 0.0092 - val_accuracy: 0.9916 - val_recall: 0.9893 - val_precision: 0.9940\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 21s 546ms/step - loss: 0.0097 - accuracy: 0.9913 - recall: 0.9895 - precision: 0.9930 - val_loss: 0.0055 - val_accuracy: 0.9958 - val_recall: 0.9946 - val_precision: 0.9968\n",
            "Epoch 1/5\n",
            "39/39 [==============================] - 33s 611ms/step - loss: 0.0831 - accuracy: 0.9422 - recall_1: 0.9074 - precision_1: 0.9703 - val_loss: 0.0285 - val_accuracy: 0.9763 - val_recall_1: 0.9753 - val_precision_1: 0.9803\n",
            "Epoch 2/5\n",
            "39/39 [==============================] - 20s 505ms/step - loss: 0.0248 - accuracy: 0.9788 - recall_1: 0.9763 - precision_1: 0.9825 - val_loss: 0.0229 - val_accuracy: 0.9795 - val_recall_1: 0.9769 - val_precision_1: 0.9871\n",
            "Epoch 3/5\n",
            "39/39 [==============================] - 20s 521ms/step - loss: 0.0174 - accuracy: 0.9836 - recall_1: 0.9799 - precision_1: 0.9902 - val_loss: 0.0153 - val_accuracy: 0.9868 - val_recall_1: 0.9826 - val_precision_1: 0.9920\n",
            "Epoch 4/5\n",
            "39/39 [==============================] - 20s 512ms/step - loss: 0.0100 - accuracy: 0.9901 - recall_1: 0.9863 - precision_1: 0.9956 - val_loss: 0.0073 - val_accuracy: 0.9930 - val_recall_1: 0.9893 - val_precision_1: 0.9982\n",
            "Epoch 5/5\n",
            "39/39 [==============================] - 20s 516ms/step - loss: 0.0064 - accuracy: 0.9936 - recall_1: 0.9917 - precision_1: 0.9967 - val_loss: 0.0058 - val_accuracy: 0.9954 - val_recall_1: 0.9930 - val_precision_1: 0.9977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model to file, so we dont have to compute it all the time again\n",
        "\"\"\"\n",
        "model_json = label_model.to_json()\n",
        "with open(\"model_1.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "label_model.save_weights(\"label_model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "model_json = intents_model.to_json()\n",
        "with open(\"model_2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "intents_model.save_weights(\"intents_model.h5\")\n",
        "print(\"Saved model to disk\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1-se1Ei4-12",
        "outputId": "58c00875-b113-45d8-ae51-8d80acf25bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to disk\n",
            "Saved model to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ9mP-IAakhK"
      },
      "outputs": [],
      "source": [
        "#Load the previously trained model\n",
        "\"\"\"\n",
        "json_file = open('model_2.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "intents_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "intents_model.load_weights(\"intents_model.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "\n",
        "json_file = open('model_1.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "label_model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "label_model.load_weights(\"label_model.h5\")\n",
        "print(\"Loaded model from disk\")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "As the models are now trained, we can analyse our models now.\n",
        "For that we have to first bring the test data into a shape with which our network works. With the on the training set created vocabulary, the test sentences are translated into numerical shape, which is then put into the model for a prediction. The prediction is applied to the whole test set, following we will look at some examples that worked well, some didn't. Additionally some plots e.g. the confusing matrix for the intents is shown."
      ],
      "metadata": {
        "id": "XSBkr5kv5RZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = []\n",
        "for x in range(len(test_texts)):\n",
        "    test_sentences.append([\" \".join(test_texts[x])])\n",
        "\n",
        "for x in range(len(test_sentences)):\n",
        "    test_sentences[x] = test_sentences[x][0]\n",
        "test_sequences = tokenizer_label.texts_to_sequences(test_sentences)\n",
        "test_data_labels = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=40)\n",
        "\n",
        "test_labels = []\n",
        "for x in range(len(test_slots)):\n",
        "    short_list = []\n",
        "    for word in (test_slots[x]).tolist():\n",
        "        try:\n",
        "            short_list.append(label2index[word])\n",
        "        except KeyError:\n",
        "            short_list.append(label2index['O'])\n",
        "    test_labels.append(short_list)\n",
        "test_labels = tf.keras.preprocessing.sequence.pad_sequences(test_labels, maxlen=40)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "\n",
        "predictions, ground_truth = make_predictions(test_data_labels, test_labels, label_model)\n",
        "\n",
        "\n",
        "trues_test = [[index2label[i] for i in sentence] for sentence in ground_truth]\n",
        "preds_test = [[index2label[i] for i in sentence] for sentence in predictions]\n"
      ],
      "metadata": {
        "id": "b8JuRXPq6mEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following example shows a perfect prediciton for the slot filling task. All labels are perfectly recognized. The majority of the labels are recognized perfectly. I didn't know a good metric for meassuring the correctness of slot filling tasks, but looking at the data manually, the accuracy is very high. Also the network gives out very good values during training.\n",
        "Note: In case you give feedback I would be interested in a way to anaylse slot filling. Simple comparing the labeled sentences didn't make sense for me, because just one wrong value doesnt mean the prediction is totally wrong."
      ],
      "metadata": {
        "id": "lRsFsiRT6vcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence:     \" + str(test_sentences[1]))\n",
        "print(\"Ground truth: \" + str(trues_test[1]))\n",
        "print(\"Prediction:   \" + str(preds_test[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cxaiz6l6vyJ",
        "outputId": "05151375-c75b-41a4-d63c-fff8adc0d14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:     on april first i need a ticket from tacoma to san jose departing before 7 am\n",
            "Ground truth: ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n",
            "Prediction:   ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(10): #\n",
        "  print(x)\n",
        "  print(\"Ground truth: \" + str(trues_test[x]))\n",
        "  print(\"Prediction:   \" + str(preds_test[x]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EItsyRCP7AJq",
        "outputId": "30f6af6b-d59d-4753-c2bb-4a4dc357b2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Ground truth: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'O', 'O', 'O', 'O', 'B-stoploc.city_name', 'I-stoploc.city_name']\n",
            "Prediction:   ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'O', 'O', 'O', 'O', 'B-stoploc.city_name', 'I-stoploc.city_name']\n",
            "1\n",
            "Ground truth: ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n",
            "Prediction:   ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n",
            "2\n",
            "Ground truth: ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n",
            "Prediction:   ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name']\n",
            "3\n",
            "Ground truth: ['O', 'O', 'O', 'O', 'O', 'O', 'B-round_trip', 'I-round_trip', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_date.month_name', 'B-depart_date.day_number']\n",
            "Prediction:   ['O', 'O', 'O', 'O', 'O', 'O', 'B-round_trip', 'I-round_trip', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_date.month_name', 'B-depart_date.day_number']\n",
            "4\n",
            "Ground truth: ['O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'B-airline_name', 'I-airline_name']\n",
            "Prediction:   ['O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'B-airline_name', 'I-airline_name']\n",
            "5\n",
            "Ground truth: ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-round_trip', 'I-round_trip', 'O', 'B-depart_date.day_name', 'B-depart_time.period_of_day', 'O', 'B-depart_date.day_name', 'B-depart_time.period_of_day']\n",
            "Prediction:   ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-round_trip', 'I-round_trip', 'O', 'B-depart_date.day_name', 'B-depart_time.period_of_day', 'B-or', 'B-depart_date.day_name', 'B-depart_time.period_of_day']\n",
            "6\n",
            "Ground truth: ['B-depart_date.day_name', 'B-depart_time.period_of_day', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
            "Prediction:   ['B-depart_date.day_name', 'B-depart_time.period_of_day', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
            "7\n",
            "Ground truth: ['O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n",
            "Prediction:   ['O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n",
            "8\n",
            "Ground truth: ['B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time', 'O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
            "Prediction:   ['B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time', 'O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
            "9\n",
            "Ground truth: ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number']\n",
            "Prediction:   ['O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'I-fromloc.city_name', 'O', 'B-toloc.city_name', 'O', 'B-depart_date.day_name', 'B-depart_date.month_name', 'B-depart_date.day_number']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following some labeling that failed:\n",
        "\n",
        "1st: Tacoma Airport is not correctly identified, but its also hard, as airport sometimes is often labeled with a 0 in the train data."
      ],
      "metadata": {
        "id": "ie4D2zAM9EEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence:     \" + str(test_sentences[32]))\n",
        "print(\"Ground truth: \" + str(trues_test[32]))\n",
        "print(\"Prediction:   \" + str(preds_test[32]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrBfi3GB7jTY",
        "outputId": "ee17e941-9187-4aca-902a-9fe031097703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:     does tacoma airport offer transportation from the airport to the downtown area\n",
            "Ground truth: ['O', 'B-airport_name', 'I-airport_name', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "Prediction:   ['O', 'B-airport_name', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd: Two flight numbers following each other. Also a hard case, as numbers always are the flight number, but in this case, the true value isn't."
      ],
      "metadata": {
        "id": "ooXNOXYb9V-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence:     \" + str(test_sentences[51]))\n",
        "print(\"Ground truth: \" + str(trues_test[51]))\n",
        "print(\"Prediction:   \" + str(preds_test[51]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biHPbdOU7jf0",
        "outputId": "83b51ee1-1124-4f12-fe77-9f512b45b249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:     what meals are served on american flight 665 673 from milwaukee to seattle\n",
            "Ground truth: ['O', 'B-meal', 'O', 'O', 'O', 'B-airline_name', 'O', 'B-flight_number', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
            "Prediction:   ['O', 'B-meal', 'O', 'O', 'O', 'B-airline_name', 'O', 'B-flight_number', 'B-flight_number', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3rd: Here the network has problems to identify the word \"this\" as a reference for the day instead of just a filler word. "
      ],
      "metadata": {
        "id": "hGORIbfJ9o9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence:     \" + str(test_sentences[70]))\n",
        "print(\"Ground truth: \" + str(trues_test[70]))\n",
        "print(\"Prediction:   \" + str(preds_test[70]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HpxJRGM7juA",
        "outputId": "4b46a4e1-99de-4bac-9bfd-e71c64fa184f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:     i would like to leave this afternoon on an american flight from cincinnati to burbank\n",
            "Ground truth: ['O', 'O', 'O', 'O', 'O', 'B-depart_date.today_relative', 'B-depart_time.period_of_day', 'O', 'O', 'B-airline_name', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name']\n",
            "Prediction:   ['O', 'O', 'O', 'O', 'O', 'O', 'B-depart_time.period_of_day', 'O', 'O', 'B-airline_name', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now look at the automated labeling from spacy. Nearly all labels are assigned correctly. City names including prefixes are recognized. Only the departure time \"before 7 am\" is not completely recognized, as the word before is ignored. Our model on the other hand recognized this word, as our model is more sensibilsed for this vocabulary."
      ],
      "metadata": {
        "id": "Lcawkd6t-S7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_sentences[1])\n",
        "print(\"Prediction: \" + str(preds_test[1]))\n",
        "\n",
        "nlp_pre = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp_pre(test_sentences[1])\n",
        "\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "t155CYtlDjEu",
        "outputId": "5e04d49f-a987-4916-b838-806cc786bd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on april first i need a ticket from tacoma to san jose departing before 7 am\n",
            "Prediction: ['O', 'B-depart_date.month_name', 'B-depart_date.day_number', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'B-toloc.city_name', 'I-toloc.city_name', 'O', 'B-depart_time.time_relative', 'B-depart_time.time', 'I-depart_time.time']\n",
            "april first DATE\n",
            "tacoma GPE\n",
            "san jose GPE\n",
            "7 am TIME\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"37c5c61bc1344a319883eeb1809f59f8-0\" class=\"displacy\" width=\"1490\" height=\"362.0\" direction=\"ltr\" style=\"max-width: none; height: 362.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">on</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">april</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">first</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">i</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">need</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">ticket</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">from</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">tacoma</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">san</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">jose</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">departing</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1220\">before</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1220\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">7</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">am</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-0\" stroke-width=\"2px\" d=\"M70,227.0 C70,92.0 400.0,92.0 400.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,229.0 L62,217.0 78,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-1\" stroke-width=\"2px\" d=\"M70,227.0 C70,182.0 120.0,182.0 120.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M120.0,229.0 L128.0,217.0 112.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-2\" stroke-width=\"2px\" d=\"M250,227.0 C250,137.0 395.0,137.0 395.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M250,229.0 L242,217.0 258,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-3\" stroke-width=\"2px\" d=\"M340,227.0 C340,182.0 390.0,182.0 390.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340,229.0 L332,217.0 348,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-4\" stroke-width=\"2px\" d=\"M520,227.0 C520,182.0 570.0,182.0 570.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520,229.0 L512,217.0 528,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-5\" stroke-width=\"2px\" d=\"M430,227.0 C430,137.0 575.0,137.0 575.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,229.0 L583.0,217.0 567.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-6\" stroke-width=\"2px\" d=\"M610,227.0 C610,182.0 660.0,182.0 660.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M660.0,229.0 L668.0,217.0 652.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-7\" stroke-width=\"2px\" d=\"M700,227.0 C700,182.0 750.0,182.0 750.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,229.0 L758.0,217.0 742.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-8\" stroke-width=\"2px\" d=\"M430,227.0 C430,47.0 855.0,47.0 855.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M855.0,229.0 L863.0,217.0 847.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-9\" stroke-width=\"2px\" d=\"M970,227.0 C970,182.0 1020.0,182.0 1020.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M970,229.0 L962,217.0 978,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-10\" stroke-width=\"2px\" d=\"M880,227.0 C880,137.0 1025.0,137.0 1025.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1025.0,229.0 L1033.0,217.0 1017.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-11\" stroke-width=\"2px\" d=\"M430,227.0 C430,2.0 1130.0,2.0 1130.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1130.0,229.0 L1138.0,217.0 1122.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-12\" stroke-width=\"2px\" d=\"M1150,227.0 C1150,182.0 1200.0,182.0 1200.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1200.0,229.0 L1208.0,217.0 1192.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-13\" stroke-width=\"2px\" d=\"M1240,227.0 C1240,182.0 1290.0,182.0 1290.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1290.0,229.0 L1298.0,217.0 1282.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-37c5c61bc1344a319883eeb1809f59f8-0-14\" stroke-width=\"2px\" d=\"M1240,227.0 C1240,137.0 1385.0,137.0 1385.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-37c5c61bc1344a319883eeb1809f59f8-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1385.0,229.0 L1393.0,217.0 1377.0,217.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the intent predictions are analysed:"
      ],
      "metadata": {
        "id": "IAhj8PEM_P2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts, test_slots, test_intents = get_data(test_file)\n",
        "test_sentences = []\n",
        "for x in range(len(test_texts)):\n",
        "    test_sentences.append([\" \".join(test_texts[x])])\n",
        "\n",
        "for x in range(len(test_sentences)):\n",
        "    test_sentences[x] = test_sentences[x][0]\n",
        "test_sequences = tokenizer_intent.texts_to_sequences(test_sentences)\n",
        "test_data_intents = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=40)\n",
        "\n",
        "test_intents_big = []\n",
        "for word in test_intents:\n",
        "    try:\n",
        "        test_intents_big.append([intent2index[word]])\n",
        "    except KeyError:\n",
        "        test_intents_big.append([intent2index['atis_flight']])\n",
        "\n",
        "test_intents = tf.keras.preprocessing.sequence.pad_sequences(test_intents_big, maxlen=40)\n",
        "test_intents = tf.keras.utils.to_categorical(test_intents)\n",
        "\n",
        "\n",
        "predictions, ground_truth = make_predictions(test_data_intents, test_intents, intents_model)\n",
        "\n",
        "\n",
        "trues_test = [[index2intent[i] for i in sentence] for sentence in ground_truth]\n",
        "preds_test = [[index2intent[i] for i in sentence] for sentence in predictions]"
      ],
      "metadata": {
        "id": "b7wtIqkc62sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"for x in range(len(trues_test)):\n",
        "  print(x)\n",
        "  #print(\"Sentence:     \" + str(test_sentences[x]))\n",
        "  print(\"Ground truth: \" + str(trues_test[x][-1]))\n",
        "  print(\"Prediction:   \" + str(preds_test[x][-1]))\"\"\""
      ],
      "metadata": {
        "id": "tpMvRnvKEsZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As previous discussed most of the intents are \"atis_flight\" with over 75% in total. Therefore I looked for special cases where it is not the intent to see how the network is performing.\n",
        "Following two good and and one bad example."
      ],
      "metadata": {
        "id": "WpHLUwrkgbhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence:     \" + str(test_sentences[103]))\n",
        "print(\"Ground truth: \" + str(trues_test[103][-1]))\n",
        "print(\"Prediction:   \" + str(preds_test[103][-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWXw8hDPE1M1",
        "outputId": "c1549259-2675-454d-8229-f061431848c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:     what is the most expensive one way fare between detroit and westchester county\n",
            "Ground truth: atis_airfare\n",
            "Prediction:   atis_airfare\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence:     \" + str(test_sentences[104]))\n",
        "print(\"Ground truth: \" + str(trues_test[104][-1]))\n",
        "print(\"Prediction:   \" + str(preds_test[104][-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG-g05LRHDGw",
        "outputId": "7757e373-c8ec-4b6e-ec9c-86193235c722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:     what airlines fly between detroit and westchester county\n",
            "Ground truth: atis_airline\n",
            "Prediction:   atis_airline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sentence:     \" + str(test_sentences[138]))\n",
        "print(\"Ground truth: \" + str(trues_test[138][-1]))\n",
        "print(\"Prediction:   \" + str(preds_test[138][-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgE0vHVIG_uQ",
        "outputId": "d278ce33-5b06-49e3-c706-0a9f3df9251e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:     what are the fares for ground transportation in denver\n",
            "Ground truth: atis_ground_fare\n",
            "Prediction:   atis_ground_service\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a better insight in TP and FP predictions it is helpful to look at the confusion matrix. Most values are on the diagonal which is good. Most error seem to appear in the row and column with the big value of 621, which is \"atis_flight\". This is not very surprising, because if most of the data is this value, its also causing most of the errors.\n",
        "Other labels seem to have a very bad prediction in general. I assume that this is due to the bias dataset and litle training data for some intents, but this will be discussed further down."
      ],
      "metadata": {
        "id": "N0iJa6Y1hHYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_preds = []\n",
        "new_trues = []\n",
        "for x in range(len(trues_test)):\n",
        "  buff = trues_test[x][-1]\n",
        "  new_trues.append(buff)\n",
        "\n",
        "for x in range(len(preds_test)):\n",
        "  buff = preds_test[x][-1]\n",
        "  new_preds.append(buff)\n",
        "print(confusion_matrix(new_trues, new_preds))\n",
        "print()\n",
        "c = Counter(new_trues)\n",
        "print(c.most_common(5))\n"
      ],
      "metadata": {
        "id": "OsbH9NV1Jk6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7ab0ba-0f6e-4a71-f575-3aa51c6ee0ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 25   0   0   0   0   3   0   0   5   0   0   0   0   0   0   0]\n",
            " [  0   4   0   0   0   3   0   0   0   0   0   1   0   0   0   1]\n",
            " [  0   0  44   0   0   0   0   0   3   0   0   1   0   0   0   0]\n",
            " [  0   0   3  35   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  1   0   0   6   5   0   0   0   5   0   0   0   1   0   0   0]\n",
            " [  0   8   4   0   0   8   0   0   0   0   0   0   1   0   0   0]\n",
            " [  1   0   0   3   0   0   0   0   2   0   0   0   0   0   0   0]\n",
            " [  0   0   1   1   0   0   0   5   0   0   0   0   3   0   0   0]\n",
            " [  2   2   5   1   0   0   0   1 621   0   0   2   1   0   0   2]\n",
            " [  0   0   1   0   0   0   0   0  11   0   0   0   0   0   0   0]\n",
            " [  0   0   6   0   0   0   0   0   1   0   0   1   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   1   0   0   0   0   5   1   0   0]\n",
            " [  0   1   0   0   0   0   0   0   0   0   0   0   0  31   0   4]\n",
            " [  0   0   0   1   0   0   0   0   5   0   0   0   0   0   0   0]\n",
            " [  1   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0]]\n",
            "\n",
            "[('atis_flight', 637), ('atis_airfare', 48), ('atis_airline', 38), ('atis_ground_service', 36), ('atis_abbreviation', 33)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall accuracy is with 88% not bad, but not increadible good either. Following some possible explanations:\n",
        "- The network is not perfect suitable for that case: As I just used the previous network build up again, parameters could be changed and increase the performance. Maybe another network could be better, e.g. a Dynamic Bayesian Network, as looking for keywords related to each other could be the key here.\n",
        "- The data is biased: With just a few hundred datapoints for each intent (beside atis_flight) it is hard for the network to learn a pattern. Especially if there is one label, that appears significantly more often than the others. The network learns the behavoir to predict that label, while the other ones are more neglected. Therefore a better distributed dataset would help to improve the performance."
      ],
      "metadata": {
        "id": "-d5zc6YqXZaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(new_trues, new_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gon5-nPZB8H4",
        "outputId": "8fa42f1a-c370-4bc9-c585-ca94fa3d7d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8779395296752519"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvM6t1RxcUk7"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "The conclusion can be divided into two parts:\n",
        "- the slot filling model works very well. Looking through the predictions, little errors can be seen. The errors don't seem to base on a fundamental learning error but just very hard cases, where ambigous words are wrongly interpretated (e.g. \"this\" is not identified as the time for \"this afternoon\" but as a filler word (e.g. \"this airport\")). I think that the dataset is also very good, as there is no label that appears significantly higher than the other ones.\n",
        "\n",
        "- The intent prediction task doesn't perform bad, but not very well either. One could expect that a simple classification process could perform better than 88%. Maybe a different network would perform better. But from my experiences I would say that the error lies mostly in the dataset, which is strongly biased to the intent \"atis_flight\", leading to a network predicting this value fine, but neglecting the other intents. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Intent slot identification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}